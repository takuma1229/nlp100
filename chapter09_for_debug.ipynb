{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpJLu1X4hgBL1fD5H+l2/o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## どうにも詰まってしまったので[こちら](https://mori-memo.hateblo.jp/entry/2022/10/16/173011)をとりあえず動かしてみる"],"metadata":{"id":"2bxkitm9a0S0"}},{"cell_type":"markdown","source":["## 80"],"metadata":{"id":"14UFktBPbEPI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxM0kDVUbH2J","executionInfo":{"status":"ok","timestamp":1698758541889,"user_tz":-540,"elapsed":19215,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"22c3094b-016c-4ccc-f45d-8a32a1cc8894"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ディレクトリの移動\n","%cd /content/drive/MyDrive/nlp100\n","%pwd\n","!ls\n","%cd ./nlp100/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLAXh86tb15m","executionInfo":{"status":"ok","timestamp":1698758542332,"user_tz":-540,"elapsed":449,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"2070fe4f-10ee-4473-bd05-3dd2d61292a2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nlp100\n"," analogy.txt\t\t  datas_ch8\t\t\t       set1.csv\n"," chapter09.ipynb\t  github.ipynb\t\t\t       set1.tab\n"," chapter6.ipynb\t\t  GoogleNews-vectors-negative300.bin   set2.csv\n"," chapter7.ipynb\t\t  instructions.txt\t\t       set2.tab\n","'chapter8 (1).ipynb'\t  NewsAggregatorDataset\t\t       sytactic_analogy.csv\n"," chapter8.ipynb\t\t  NewsAggregatorDatasettest.txt        test_feature.txt\n"," chapter9_archive.ipynb   NewsAggregatorDatasettrain.txt       train_feature.txt\n"," combined.csv\t\t  NewsAggregatorDatasetval.txt\t       valid_feature.txt\n"," combined.tab\t\t  nlp100\t\t\t       wordsim353\n"," countries.csv\t\t  semantic_analogy.csv\t\t       wordsim353.zip\n","/content/drive/MyDrive/nlp100/nlp100\n"]}]},{"cell_type":"code","source":["# データのロード\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# ファイル読み込み\n","file = \"../NewsAggregatorDataset/newsCorpora.csv\"\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# データ数の確認\n","print('学習データ')\n","print(train['CATEGORY'].value_counts())\n","print('検証データ')\n","print(valid['CATEGORY'].value_counts())\n","print('評価データ')\n","print(test['CATEGORY'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtxF527ra3DA","executionInfo":{"status":"ok","timestamp":1698758549659,"user_tz":-540,"elapsed":7329,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"1136f011-79fc-42c3-9280-4e51f1d8e453"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["学習データ\n","b    4502\n","e    4223\n","t    1219\n","m     728\n","Name: CATEGORY, dtype: int64\n","検証データ\n","b    562\n","e    528\n","t    153\n","m     91\n","Name: CATEGORY, dtype: int64\n","評価データ\n","b    563\n","e    528\n","t    152\n","m     91\n","Name: CATEGORY, dtype: int64\n"]}]},{"cell_type":"code","source":["# 単語の辞書を作成\n","from collections import Counter\n","words = []\n","for text in train['TITLE']:\n","    for word in text.rstrip().split():\n","        words.append(word)\n","c = Counter(words)\n","word2id = {}\n","for i, cnt in enumerate(c.most_common()):\n","    if cnt[1] > 1:\n","        word2id[cnt[0]] = i + 1\n","for i, cnt in enumerate(word2id.items()):\n","    if i >= 10:\n","        break\n","    print(cnt[0], cnt[1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unvLVssNa3BJ","executionInfo":{"status":"ok","timestamp":1698758549659,"user_tz":-540,"elapsed":5,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"f2137593-a1f8-4ecb-aee7-6a358db2d5e7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["to 1\n","0 2\n","in 3\n","as 4\n","on 5\n","UPDATE 6\n","for 7\n","The 8\n","of 9\n","US 10\n"]}]},{"cell_type":"code","source":["# 単語のID化\n","def tokenizer(text):\n","    words = text.rstrip().split()\n","    return [word2id.get(word, 0) for word in words]\n","\n","sample = train.at[0, 'TITLE']\n","print(sample)\n","print(tokenizer(sample))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7UrvoUea2_Q","executionInfo":{"status":"ok","timestamp":1698758549659,"user_tz":-540,"elapsed":3,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"b8b6d143-22ec-4d6b-98db-b3f304d06f67"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Justin Bieber Under Investigation For Attempted Robbery At Dave  Busters\n","[68, 76, 782, 1974, 21, 5054, 5055, 34, 1602, 0]\n"]}]},{"cell_type":"markdown","source":["## 81"],"metadata":{"id":"LaQtBiFJdZv9"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","VOCAB_SIZE = 4\n","EMB_SIZE = 3\n","emb = nn.Embedding(VOCAB_SIZE, EMB_SIZE)\n","\n","words = torch.tensor([1, 3, 0, 2, 1, 2])\n","embed_words = emb(words)\n","print(embed_words)\n","print(words.shape, '->', embed_words.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddujtv9Ja29B","executionInfo":{"status":"ok","timestamp":1698758554924,"user_tz":-540,"elapsed":5267,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"e754306e-7c2b-45ed-81f1-a02bb16b3848"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0076, -1.0310,  1.1935],\n","        [-0.3319, -1.1299, -0.5302],\n","        [ 0.6527, -1.4976,  0.2532],\n","        [-1.2356,  1.4862, -1.6536],\n","        [-0.0076, -1.0310,  1.1935],\n","        [-1.2356,  1.4862, -1.6536]], grad_fn=<EmbeddingBackward0>)\n","torch.Size([6]) -> torch.Size([6, 3])\n"]}]},{"cell_type":"code","source":["# RNNの作成\n","# モデルの構築\n","import random\n","import torch\n","from torch import nn\n","import torch.utils.data as data\n","\n","# 乱数のシードを設定\n","# parserなどで指定\n","seed = 1234\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(seed)\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.rnn = nn.LSTM(emb_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, h0=None):\n","        x = self.emb(x)\n","        x, h = self.rnn(x, h0)\n","        x = x[:, -1, :]\n","        logits = self.fc(x)\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values())) + 1\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","NUM_LAYERS = 1\n","\n","# モデルの定義\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zcwkqq64a26y","executionInfo":{"status":"ok","timestamp":1698758554924,"user_tz":-540,"elapsed":3,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"e5f73933-0672-4d90-c298-3e53b4cddf01"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (emb): Embedding(9725, 300, padding_idx=9724)\n","  (rnn): LSTM(300, 50, batch_first=True)\n","  (fc): Linear(in_features=50, out_features=4, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["# 初期状態での推論\n","x = torch.tensor([tokenizer(sample)], dtype=torch.int64)\n","print(x)\n","print(x.size())\n","print(nn.Softmax(dim=-1)(model(x)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSe6-fPia24w","executionInfo":{"status":"ok","timestamp":1698758555550,"user_tz":-540,"elapsed":628,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"88924db9-8aec-434c-fd36-f062dd7077d9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  68,   76,  782, 1974,   21, 5054, 5055,   34, 1602,    0]])\n","torch.Size([1, 10])\n","tensor([[0.2783, 0.2129, 0.2804, 0.2284]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["## 82"],"metadata":{"id":"9-dR9XJJdpZz"}},{"cell_type":"code","source":["# ターゲットのテンソル化\n","category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n","Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n","Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n","Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n","print(Y_train.size())\n","print(Y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7EETYQfa22g","executionInfo":{"status":"ok","timestamp":1698758555550,"user_tz":-540,"elapsed":5,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"1f0a3e5b-1609-4e4b-9a83-97e01ba5f41d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10672])\n","tensor([2, 0, 2,  ..., 0, 0, 0])\n"]}]},{"cell_type":"code","source":["class NewsDataset(data.Dataset):\n","    \"\"\"\n","    newsのDatasetクラス\n","\n","    Attributes\n","    ----------------------------\n","    X : データフレーム\n","        単語ベクトルの平均をまとめたテンソル\n","    y : テンソル\n","        カテゴリをラベル化したテンソル\n","    phase : 'train' or 'val'\n","        学習か訓練かを設定する\n","    \"\"\"\n","    def __init__(self, X, y, phase='train'):\n","        self.X = X['TITLE']\n","        self.y = y\n","        self.phase = phase\n","\n","    def __len__(self):\n","        \"\"\"全データサイズを返す\"\"\"\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"idxに対応するテンソル形式のデータとラベルを取得\"\"\"\n","        inputs = torch.tensor(tokenizer(self.X[idx]))\n","        return inputs, self.y[idx]\n","\n","train_dataset = NewsDataset(train, Y_train, phase='train')\n","valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n","test_dataset = NewsDataset(test, Y_test, phase='val')\n","# 動作確認\n","idx = 0\n","print(train_dataset.__getitem__(idx)[0].size())\n","print(train_dataset.__getitem__(idx)[1])\n","print(valid_dataset.__getitem__(idx)[0].size())\n","print(valid_dataset.__getitem__(idx)[1])\n","print(test_dataset.__getitem__(idx)[0].size())\n","print(test_dataset.__getitem__(idx)[1])\n","\n","print(train)\n","print(type(train_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVyeltsia20p","executionInfo":{"status":"ok","timestamp":1698760477008,"user_tz":-540,"elapsed":3,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"42e92d80-3cd7-428d-e684-75b4ea9419fa"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10])\n","tensor(2)\n","torch.Size([11])\n","tensor(3)\n","torch.Size([13])\n","tensor(2)\n","                                                   TITLE CATEGORY\n","0      Justin Bieber Under Investigation For Attempte...        e\n","1      Exxon Report Claims World Highly Unlikely To L...        b\n","2      Jack White Records Releases Single In Hours Fo...        e\n","3      President Barack Obama Releases Proclamation D...        t\n","4      Samsung Shares Steady After Chairmans Heart At...        m\n","...                                                  ...      ...\n","10667  JK Rowling Brings Her Magic To TV HBO And BBC ...        e\n","10668  UPDATE 0-Peace Corps pulls volunteers from Wes...        m\n","10669  GRAINS-US soybean prices climb on rising Chine...        b\n","10670  Seafood Fraud Under Fire As Lawmakers Look To ...        b\n","10671  A Hedge Fund Wants to Teach PetSmart Some New ...        b\n","\n","[10672 rows x 2 columns]\n","<class '__main__.NewsDataset'>\n"]}]},{"cell_type":"code","source":["print(train_dataset.__len__())\n","print(train_dataset.__getitem__(0))\n","\n","#train_datasetはtorch.utils.data.Datasetクラス\n","    #自分のデータもdataset.Datasetクラスにする"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izcbcNrp59sv","executionInfo":{"status":"ok","timestamp":1698759010753,"user_tz":-540,"elapsed":2,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"9aee17a4-c157-4c52-8ff1-a3c73621b024"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["10672\n","(tensor([  68,   76,  782, 1974,   21, 5054, 5055,   34, 1602,    0]), tensor(2))\n"]}]},{"cell_type":"code","source":["# DataLoaderを作成\n","batch_size = 1\n","\n","train_dataloader = data.DataLoader(\n","            train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","valid_dataloader = data.DataLoader(\n","            valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","test_dataloader = data.DataLoader(\n","            test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","dataloaders_dict = {'train': train_dataloader,\n","                    'val': valid_dataloader,\n","                    'test': test_dataloader,\n","                   }\n","\n","# 動作確認\n","batch_iter = iter(dataloaders_dict['train'])\n","inputs, labels = next(batch_iter)\n","print(inputs.size())\n","print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSln6XzIa2yt","executionInfo":{"status":"ok","timestamp":1698758555551,"user_tz":-540,"elapsed":4,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"55c21be9-09c3-4dce-97c1-06658e24fa14"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 11])\n","tensor([2])\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# 学習用の関数を定義\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    train_loss = []\n","    train_acc = []\n","    valid_loss = []\n","    valid_acc = []\n","    # epochのループ\n","    for epoch in range(num_epochs):\n","        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n","        print('--------------------------------------------')\n","\n","        # epochごとの学習と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train() # 訓練モード\n","            else:\n","                net.eval() # 検証モード\n","\n","            epoch_loss = 0.0 # epochの損失和\n","            epoch_corrects = 0 # epochの正解数\n","\n","            # データローダーからミニバッチを取り出すループ\n","            for inputs, labels in tqdm(dataloaders_dict[phase]):\n","                optimizer.zero_grad() # optimizerを初期化\n","\n","                # 順伝播計算(forward)\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(inputs)\n","                    loss = criterion(outputs, labels) # 損失を計算\n","                    _, preds = torch.max(outputs, 1) # ラベルを予想\n","\n","                    # 訓練時は逆伝播\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                    # イテレーション結果の計算\n","                    # lossの合計を更新\n","                    epoch_loss += loss.item() * inputs.size(0)\n","                    # 正解数の合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # epochごとのlossと正解率の表示\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n","            if phase == 'train':\n","                train_loss.append(epoch_loss)\n","                train_acc.append(epoch_acc)\n","            else:\n","                valid_loss.append(epoch_loss)\n","                valid_acc.append(epoch_acc)\n","\n","            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","    return train_loss, train_acc, valid_loss, valid_acc\n","\n","# 学習を実行する\n","\n","# モデルの定義\n","net = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n","net.train()\n","\n","# 損失関数の定義\n","criterion = nn.CrossEntropyLoss()\n","\n","# 最適化手法の定義\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","num_epochs = 5\n","train_loss, train_acc, valid_loss, valid_acc = train_model(net,\n","            dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idYnvTWva2wI","executionInfo":{"status":"ok","timestamp":1698685130965,"user_tz":-540,"elapsed":252698,"user":{"displayName":"T S","userId":"17702794001545199541"}},"outputId":"6380b2e9-1d6f-4fac-aff8-c19225f24000"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10672/10672 [00:52<00:00, 204.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.9959, Acc: 0.6041\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [00:00<00:00, 1903.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.7686, Acc: 0.7391\n","Epoch 2 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10672/10672 [00:50<00:00, 211.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.6217, Acc: 0.7737\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [00:00<00:00, 1936.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.6238, Acc: 0.7729\n","Epoch 3 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10672/10672 [00:49<00:00, 216.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.3893, Acc: 0.8621\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [00:00<00:00, 1883.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.5844, Acc: 0.8021\n","Epoch 4 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10672/10672 [00:48<00:00, 217.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.2284, Acc: 0.9231\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [00:00<00:00, 1924.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.5823, Acc: 0.8096\n","Epoch 5 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10672/10672 [00:48<00:00, 220.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1206, Acc: 0.9639\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [00:00<00:00, 1934.03it/s]"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.6249, Acc: 0.8193\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["- ↑そもそもなんか学習時間がこっちのほうが圧倒的に短いので、データに問題がある気もする\n","    - padding周りだったりするかなー\n","    - 学習の精度もちゃんと(概ね参考記事どおり)出てる。"],"metadata":{"id":"rSLo27pXd47y"}},{"cell_type":"code","source":[],"metadata":{"id":"W0eXhuGga2uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cGQxencMa2sB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DL-alKXma2px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IJYdxZiOa2n5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yfAMfGEna2lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1ZuS8BXha2jZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hdOx7ZNza2hh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7qAINhvPa2fg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vCgA_cwDa2dR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"soctjKH1a2bB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XeeWupV9a2Y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMdOo7IxanGv"},"outputs":[],"source":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"aRMVk3kBasR5"}}]}